# -*- coding: utf-8 -*-
"""LAB3_weather_YARA (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o0_RpzbFGlH6BSMKJln-LR1Ba7YpVuNi

# TASK 1:  Write the data science process steps
1. scope the problem
2. collect data
3. import libraries
4. load data
5. basic preprocessing
6. type conversion and check for null values
7. visualization (EDA)
8. advanced preprocessing: feature engineering
9. machine learning

# Task 2: Data Collection
"""

# import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report
from google.colab import drive
drive.mount('/content/drive')

# load dataset and display first 5 rows
df = pd.read_csv('/content/drive/MyDrive/ECE 333/LAB 3/weatherHistory.csv')
df.head()

"""# Task 3: Data Preprocessing

"""

# missing values
df.isnull().sum()

# data types of each column
df.dtypes

# unique values in the target column
df['Precip Type'].value_counts()

df['Precip Type'].describe()

# how many rows and columns are in the dataset?
df.shape

# what are the unique precipitation types?
df['Precip Type'].unique()

# fill missing values in Precip Type with "none"
df['Precip Type'].fillna('none',inplace=True)

df['Precip Type'].isnull().sum()

# encode Precip Type into numeric labels (LabelEncoder)
le = LabelEncoder()
df['Precip Type Encoded'] = le.fit_transform(df['Precip Type'].astype(str))
label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
print(label_mapping)

# drop irrelevant columns
df.drop(['Formatted Date','Precip Type', 'Summary', 'Daily Summary', 'Loud Cover'], axis=1, inplace=True)
print(df.columns)

# apply feature scaling (StandardScaler)
sc = StandardScaler()
scaled_data = sc.fit_transform(df)
df_scaled = pd.DataFrame(scaled_data, columns=df.columns)
print(df_scaled.head())

"""# Task 4: Learning"""

# define X (features) and y (target)
X = df.drop('Precip Type Encoded', axis=1)
y = df['Precip Type Encoded']

# split data into 80% training and 20% testing sets using train_test_split
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42, stratify=y)
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#  train and compare three classifiers:
rf = RandomForestClassifier(random_state=42)
gb = GradientBoostingClassifier(random_state=42)
xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)

rf.fit(X_train, y_train)
gb.fit(X_train, y_train)
xgb.fit(X_train, y_train)


rf_pred = rf.predict(X_test)
gb_pred = gb.predict(X_test)
xgb_pred = xgb.predict(X_test)

print("Random Forest Accuracy:", accuracy_score(y_test, rf_pred))
print("Gradient Boosting Accuracy:", accuracy_score(y_test, gb_pred))
print("XGBoost Accuracy:", accuracy_score(y_test, xgb_pred))

print("\nRandom Forest Report:\n", classification_report(y_test, rf_pred))
print("\nGradient Boosting Report:\n", classification_report(y_test, gb_pred))
print("\nXGBoost Report:\n", classification_report(y_test, xgb_pred))

"""# Task 5: Reflection

random forest, gradient boosting, and xgboost all gave very high overall accuracy on the weather dataset, but their handling of the rare class (which is 'none' in my case) was different: random forest uses bagging, less prone to overfitting, but less powerful than boosting methods. its also easier to interpret. gradient boosting builds trees sequentially to correct residuals, but xgboost adds regularization, handles missing values by learning split directions, and is designed for speed/efficiency.


since real-world weather prediction often deals with imbalanced data and missing values, i would recommend xgboost, as it is faster and more efficient than traditional boosting and also better at handling missing values, which is particularly useful in weather datasets

source:https://www.geeksforgeeks.org/machine-learning/difference-between-random-forest-vs-xgboost/
"""